{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://drive.google.com/uc?id=1Z3JvAFmL2IkBnQmmt5f4uTcXVhO5f7cq\"/></center>\n",
    "\n",
    "------\n",
    "<center>&copy; Research Group CAMMA, University of Strasbourg, <a href=\"http://camma.u-strasbg.fr\">http://camma.u-strasbg.fr</a>\n",
    "\n",
    "<h2>Authors: Vinkle Srivastav, Idris Hamoud, Ege Oszoy </h2>\n",
    "</center>\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=green> Lecture: Human Pose Estimation in the Operating Room </font></center>\n",
    "<center><img src=\"https://lh3.googleusercontent.com/drive-viewer/AFGJ81qU1PeDDhk9Tl0tfMwk9pMOGEz__WBAXMU6Fo4hZXgz20jwymqYgGL4PS6J0BxihQIeRmsv3nR14cgszU2YLKcYV6AefA=s1600\"/></center>\n",
    "\n",
    "\n",
    "### **Objectives**:\n",
    "  1. Understand how Pose estimation can be used to understand body language and non-verbal communication cues. It can be used in healthcare for patient monitoring or in this case in the Operating Room for workflow monitoring.\n",
    "  2. PyTorch `Dataset` and `Dataloader` for subset of MVOR dataset\n",
    "  3. Run off-the-shelf top-down approaches for HPE using Cascade Mask R-CNN as an object detector, and compare with groundtruth bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install external dependencies\n",
    "# install dependencies\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install torch\n",
    "# !pip install torchvision\n",
    "# !pip install tqdm\n",
    "# !pip install ipywidgets\n",
    "# !pip install -U openmim\n",
    "# !mim install mmengine\n",
    "# !mim install \"mmcv>=2.0.0\"\n",
    "# !mim install \"mmdet>=3.0.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download resources\n",
    "ROOT_DIR = \"./HPE_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egeozsoy/venvs/EDU4SDS23/lib/python3.7/site-packages/mmcv/cnn/bricks/transformer.py:28: UserWarning: Fail to import ``MultiScaleDeformableAttention`` from ``mmcv.ops.multi_scale_deform_attn``, You should install ``mmcv-full`` if you need this module. \n",
      "  warnings.warn('Fail to import ``MultiScaleDeformableAttention`` from '\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/egeozsoy/venvs/EDU4SDS23/lib/python3.7/site-packages/mmcv/_ext.cpython-37m-darwin.so, 0x0002): Symbol not found: __Z16THPVariable_WrapN2at6TensorE\n  Referenced from: <361BFD06-3F0D-3300-88B3-9C796D1F5279> /Users/egeozsoy/venvs/EDU4SDS23/lib/python3.7/site-packages/mmcv/_ext.cpython-37m-darwin.so\n  Expected in:     <AA69D6A3-7D5A-3F97-B7B9-F6F15CFF01B4> /Users/egeozsoy/venvs/EDU4SDS23/lib/python3.7/site-packages/torch/lib/libtorch_python.dylib",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/fy/v577syx96qs8hqmff2g5qnk00000gn/T/ipykernel_4090/4014365721.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     28\u001B[0m from mmpose.apis import (inference_top_down_pose_model, inference_bottom_up_pose_model,\n\u001B[1;32m     29\u001B[0m                          vis_pose_result, process_mmdet_results, init_pose_model)\n\u001B[0;32m---> 30\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mmmdet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapis\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0minference_detector\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minit_detector\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpycocotools\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcoco\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mCOCO\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[0;31m#from mmpose.datasets import DatasetInfo\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/venvs/EDU4SDS23/lib/python3.7/site-packages/mmdet/apis/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Copyright (c) OpenMMLab. All rights reserved.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m from .inference import (async_inference_detector, inference_detector,\n\u001B[0m\u001B[1;32m      3\u001B[0m                         init_detector, show_result_pyplot)\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mtest\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmulti_gpu_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msingle_gpu_test\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m from .train import (get_root_logger, init_random_seed, set_random_seed,\n",
      "\u001B[0;32m~/venvs/EDU4SDS23/lib/python3.7/site-packages/mmdet/apis/inference.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mmmcv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mops\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mRoIPool\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mmmcv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallel\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcollate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscatter\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mmmcv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrunner\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mload_checkpoint\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/venvs/EDU4SDS23/lib/python3.7/site-packages/mmcv/ops/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Copyright (c) OpenMMLab. All rights reserved.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0massign_score_withk\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0massign_score_withk\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mball_query\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mball_query\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mbbox\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mbbox_overlaps\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mborder_align\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mBorderAlign\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mborder_align\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/venvs/EDU4SDS23/lib/python3.7/site-packages/mmcv/ops/assign_score_withk.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m ext_module = ext_loader.load_ext(\n\u001B[0;32m----> 6\u001B[0;31m     '_ext', ['assign_score_withk_forward', 'assign_score_withk_backward'])\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/venvs/EDU4SDS23/lib/python3.7/site-packages/mmcv/utils/ext_loader.py\u001B[0m in \u001B[0;36mload_ext\u001B[0;34m(name, funcs)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mload_ext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfuncs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m         \u001B[0mext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimportlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimport_module\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'mmcv.'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mfun\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfuncs\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mext\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfun\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34mf'{fun} miss in module {name}'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/__init__.py\u001B[0m in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    126\u001B[0m             \u001B[0mlevel\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 127\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_bootstrap\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_gcd_import\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlevel\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpackage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    128\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mImportError\u001B[0m: dlopen(/Users/egeozsoy/venvs/EDU4SDS23/lib/python3.7/site-packages/mmcv/_ext.cpython-37m-darwin.so, 0x0002): Symbol not found: __Z16THPVariable_WrapN2at6TensorE\n  Referenced from: <361BFD06-3F0D-3300-88B3-9C796D1F5279> /Users/egeozsoy/venvs/EDU4SDS23/lib/python3.7/site-packages/mmcv/_ext.cpython-37m-darwin.so\n  Expected in:     <AA69D6A3-7D5A-3F97-B7B9-F6F15CFF01B4> /Users/egeozsoy/venvs/EDU4SDS23/lib/python3.7/site-packages/torch/lib/libtorch_python.dylib"
     ]
    }
   ],
   "source": [
    "#@title Imports\n",
    "import os\n",
    "#import sys\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "from torch import optim\n",
    "#import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "#from PIL import Image\n",
    "import cv2\n",
    "#import pickle\n",
    "import random\n",
    "import ipywidgets as wd\n",
    "#import glob\n",
    "#import io\n",
    "import mmcv\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "from mmcv import Config\n",
    "from mmpose.apis import (inference_top_down_pose_model, inference_bottom_up_pose_model,\n",
    "                         vis_pose_result, process_mmdet_results, init_pose_model)\n",
    "from mmdet.apis import inference_detector, init_detector\n",
    "from pycocotools.coco import COCO\n",
    "#from mmpose.datasets import DatasetInfo\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "torch.backends.cudnn.enabled=False\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "### HPE helpers\n",
    "We will in a first step visualize the keypoint annotations on one sample image from the MVOR dataset. This example will allow us to better understand the different helper functions from the mmpose library and get a better understanding of the python visualization libraries like matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the day and the name of the image we use for visualization\n",
    "sample_frame_info = (\"day1\", \"000013.png\")\n",
    "_,axs = plt.subplots(1,3,dpi=300)\n",
    "# We will plot the multiview images from the three different cameras\n",
    "for cam_id in range(3):\n",
    "    rgb_img_path = os.path.join(ROOT_DIR, \"camma_mvor_dataset\", sample_frame_info[0],\n",
    "                            \"cam\"+str(cam_id+1),\"color\", sample_frame_info[1])\n",
    "    rgb_img = mmcv.imread(rgb_img_path, channel_order='rgb')\n",
    "    axs[cam_id].imshow(rgb_img); axs[cam_id].set_title(\"Camera \"+str(cam_id+1))\n",
    "    axs[cam_id].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for visualization of depth maps\n",
    "def normalize_depth_for_display(depth, pc=60, crop_percent=0., normalizer=None, cmap='viridis'):\n",
    "    # convert to disparity\n",
    "    depth = 1./(depth + 1e-6)\n",
    "    if normalizer is not None:\n",
    "        depth = depth/normalizer\n",
    "    else:\n",
    "        depth = depth/(np.percentile(depth, pc) + 1e-6)\n",
    "    depth = np.clip(depth, 0, 1)\n",
    "    depth = rgb2gray(depth)\n",
    "    keep_H = int(depth.shape[0] * (1-crop_percent))\n",
    "    depth = depth[:keep_H]\n",
    "\n",
    "    depth = depth\n",
    "    return depth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the day and the name of the image we use for visualization\n",
    "sample_frame_info = (\"day1\", \"000013.png\")\n",
    "_,axs = plt.subplots(1,3,dpi=300)\n",
    "# We will plot the multiview images from the three different cameras\n",
    "for cam_id in range(3):\n",
    "    depth_img_path = os.path.join(ROOT_DIR, \"camma_mvor_dataset\", sample_frame_info[0],\n",
    "                            \"cam\"+str(cam_id+1),\"depth\", sample_frame_info[1])\n",
    "    depth_img = mmcv.imread(depth_img_path, channel_order='rgb')\n",
    "    depth_img = normalize_depth_for_display(depth_img)\n",
    "    axs[cam_id].imshow(depth_img,'jet'); axs[cam_id].set_title(\"Camera \"+str(cam_id+1))\n",
    "    axs[cam_id].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations from json file\n",
    "annot_path = os.path.join(ROOT_DIR, \"camma_mvor_2018.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-down approaches\n",
    "### Using off-the-shelf human detector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the human detector using mmdet api\n",
    "det_config = 'configs/cascade_rcnn_x101_64x4d_fpn_coco.py'\n",
    "pose_config = 'configs/hrnet_w48_coco_384x288.py'\n",
    "det_model = init_detector(det_config, checkpoint = 'https://download.openmmlab.com/mmdetection/v2.0/cascade_rcnn/cascade_mask_rcnn_x101_64x4d_fpn_1x_coco/cascade_mask_rcnn_x101_64x4d_fpn_1x_coco_20200203-9a2db89d.pth',device='cpu') \n",
    "pose_model = init_pose_model(pose_config, checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_coco_384x288-314c8528_20200708.pth',device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img_path =  os.path.join(ROOT_DIR, \"camma_mvor_dataset\", sample_frame_info[0],\n",
    "                          \"cam\"+str(1),\"color\", sample_frame_info[1]) #'/raid/coco/images/test2017/000000000725.jpg'\n",
    "rgb_img = mmcv.imread(rgb_img_path, channel_order='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a single image, the resulting box is (x1, y1, x2, y2)\n",
    "mmdet_results = inference_detector(det_model, rgb_img)\n",
    "# keep the person class bounding boxes and process the results in an np.array format.\n",
    "person_results = process_mmdet_results(mmdet_results, cat_id=1)\n",
    "#person_results = [{'bbox':np.array(box['bbox']), 'device':'cpu'} for box in person_results]\n",
    "#print(person_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_results, returned_outputs = inference_top_down_pose_model(\n",
    "        pose_model,\n",
    "        rgb_img,\n",
    "        person_results,\n",
    "        bbox_thr=0.,\n",
    "        format='xyxy',\n",
    "        return_heatmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_result = vis_pose_result(\n",
    "        pose_model,\n",
    "        rgb_img,\n",
    "        pose_results,\n",
    "        out_file=os.path.join(ROOT_DIR,\"test_pose.png\"))\n",
    "\n",
    "plt.imshow(vis_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(1,3,dpi=300)\n",
    "for cam_id in range(3):\n",
    "    rgb_img_path = os.path.join(ROOT_DIR, \"camma_mvor_dataset\", sample_frame_info[0],\n",
    "                            \"cam\"+str(cam_id+1),\"color\", sample_frame_info[1])\n",
    "    rgb_img = mmcv.imread(rgb_img_path, channel_order='rgb')\n",
    "    # test a single image, the resulting box is (x1, y1, x2, y2)\n",
    "    mmdet_results = inference_detector(det_model, rgb_img)\n",
    "    # keep the person class bounding boxes and process the results in an np.array format.\n",
    "    person_results = process_mmdet_results(mmdet_results, 1)\n",
    "    #person_results = [{'bbox':np.array(box['bbox'].cpu()), 'device':'cpu'} for box in person_results]\n",
    "    pose_results, returned_outputs = inference_top_down_pose_model(\n",
    "        pose_model,\n",
    "        rgb_img_path,\n",
    "        person_results,\n",
    "        bbox_thr=0.,\n",
    "        format='xyxy',\n",
    "        return_heatmap=True)\n",
    "    vis_result = vis_pose_result(\n",
    "            pose_model,\n",
    "            rgb_img,\n",
    "            pose_results)\n",
    "    axs[cam_id].imshow(vis_result); axs[cam_id].set_title(\"Camera \"+str(cam_id+1))\n",
    "    axs[cam_id].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annot_path) as f:\n",
    "   data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize COCO api for instance annotations\n",
    "coco=COCO(annot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using groundtruth bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to find ids for specific filename\n",
    "def find_id(annots_gt, file_name):\n",
    "    for img in annots_gt.imgs.values():\n",
    "        if img['file_name']==file_name:\n",
    "            return img['id']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations for sample image defined above\n",
    "id_img = find_id(coco, sample_frame_info[0] + \"/cam\"+str(cam_id+1)+\"/color/\"+ sample_frame_info[1])\n",
    "anns = coco.getAnnIds(id_img)\n",
    "person_results = [{'bbox':ann['bbox']+[1.0]} for ann in coco.loadAnns(anns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_results, returned_outputs = inference_top_down_pose_model(\n",
    "        pose_model,\n",
    "        rgb_img,\n",
    "        person_results,\n",
    "        bbox_thr=0.1,\n",
    "        format='xywh',\n",
    "        return_heatmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_result = vis_pose_result(\n",
    "        pose_model,\n",
    "        rgb_img,\n",
    "        pose_results,\n",
    "        out_file=os.path.join(ROOT_DIR,\"test_pose.png\"))\n",
    "\n",
    "plt.imshow(vis_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returned_outputs[0]['heatmap'].shape\n",
    "rgb_img.shape\n",
    "returned_outputs[0].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTS = {\n",
    "    0: 'NOSE',\n",
    "    1: 'LEFT_EYE',\n",
    "    2: 'RIGHT_EYE',\n",
    "    3: 'LEFT_EAR',\n",
    "    4: 'RIGHT_EAR',\n",
    "    5: 'LEFT_SHOULDER',\n",
    "    6: 'RIGHT_SHOULDER',\n",
    "    7: 'LEFT_ELBOW',\n",
    "    8: 'RIGHT_ELBOW',\n",
    "    9: 'LEFT_WRIST',\n",
    "    10: 'RIGHT_WRIST',\n",
    "    11: 'LEFT_HIP',\n",
    "    12: 'RIGHT_HIP',\n",
    "    13: 'LEFT_KNEE',\n",
    "    14: 'RIGHT_KNEE',\n",
    "    15: 'LEFT_ANKLE',\n",
    "    16: 'RIGHT_ANKLE'\n",
    "}\n",
    "\n",
    "for joint in PARTS:\n",
    "    heatmap = returned_outputs[0]['heatmap'][0,joint]\n",
    "    scores = 1/(1 + np.exp(-heatmap))\n",
    "    x,y = np.unravel_index(np.argmax(scores[:,:]), scores[:,:].shape)\n",
    "    print(x,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune model on MVOR ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "valid_dataset = eval('dataset.'+cfg.DATASET.DATASET)(\n",
    "    cfg, cfg.DATASET.ROOT, cfg.DATASET.TEST_SET, False,\n",
    "transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    ")\n",
    "# valid_loader = torch.utils.data.DataLoader(\n",
    "#     valid_dataset,\n",
    "#     batch_size=cfg.TEST.BATCH_SIZE_PER_GPU*len(cfg.GPUS),\n",
    "#     shuffle=False,\n",
    "#     num_workers=cfg.WORKERS,\n",
    "#     pin_memory=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
